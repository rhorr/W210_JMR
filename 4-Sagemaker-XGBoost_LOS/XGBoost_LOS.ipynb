{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96c009-8003-45a7-bbd1-b208394e12ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### XGBoost_Multi Build and Testing Local - Length of Stay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ad69c-d333-48cf-9414-1c94394f4023",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f7911-35db-4eb1-ac06-e5c9042d165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing key packages\n",
    "import io\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "import botocore\n",
    "from sagemaker import get_execution_role, image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from time import gmtime, strftime\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7fcf48-a3ea-4a9e-af45-3fa29fbf6804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "sess = sagemaker.Session(boto_session=boto_session)\n",
    "\n",
    "print(\"Role:\", role)\n",
    "print(\"Region:\", region)\n",
    "print(\"SageMaker Session Region:\", sess.boto_region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a73edc-dc84-412c-8689-8b4ddc3816a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up Bucket Links/Info\n",
    "bucket='xgb-los-multi'\n",
    "s3_bucket_prefix= \"xgb-los-model-code/\"\n",
    "prefix = f\"{bucket}/{s3_bucket_prefix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74e05e-2f58-4810-a3ca-b225cd3ce144",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad4a77-c03b-4bbf-9c0f-98d77d452243",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Set XGB Container\n",
    "\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.7-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b822194-e428-4729-98e7-b4e002c42eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fefffc8a-83aa-42a2-afd6-91669f28683c",
   "metadata": {},
   "source": [
    "## Train/Test/Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f0ed5-4f02-47ab-8a18-3541cfb1a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Source file to do encoding and split train/test\n",
    "\n",
    "#s3://sagemaker-us-east-2-917456409349/sagemaker/adoption/golden_record/df_cat_dog_harmonized_Sample_With_Outcome.csv\n",
    "\n",
    "bucket = \"sagemaker-us-east-2-917456409349\"\n",
    "key = \"sagemaker/adoption/golden_record/df_cat_dog_harmonized.csv\" \n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# Fetch the object from S3\n",
    "obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "\n",
    "# Read into pandas DataFrame\n",
    "df = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "\n",
    "# Show the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd5700d-7bc7-4636-a08b-a7f4cb729392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deduping\n",
    "\n",
    "# Drop duplicates, keeping the last record for each animal_id\n",
    "df_deduped = df.drop_duplicates(subset='primary_key', keep='last')\n",
    "\n",
    "\n",
    "print(\"Original rows:\", len(df))\n",
    "print(\"After deduplication:\", len(df_deduped))\n",
    "df = df_deduped.copy()\n",
    "print(\"New rows for df:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f0b28-5a1d-4d2b-ae27-8295aa479bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Custom Train/Test/Split\n",
    "def assign_split(row):\n",
    "    if row['outcome_year'] <= 2022:\n",
    "        return \"train\"\n",
    "    elif row['outcome_year'] in [2023, 2024]:\n",
    "        return \"validate\"\n",
    "    elif row['outcome_year'] == 2025:\n",
    "        return \"test\"\n",
    "    else:\n",
    "        return \"exclude\"  # fallback for unexpected years\n",
    "\n",
    "df['split'] = df.apply(assign_split, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f23f0-c798-4c3f-9b3d-9ecf2f9cd849",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Naming features to keep and drop if needed, but won't as keeping standard format of xlsx.\n",
    "features_to_keep = ['outcome_type_harmonized_grouped','animal_type', 'primary_breed_harmonized', 'primary_color_harmonized',\n",
    "    'sex', 'intake_type_harmonized',\n",
    "    'Is_returned', 'has_name', 'is_mix', 'Num_returned', 'age_months','stay_length_days', 'min_height', 'max_height',\n",
    "    'min_weight', 'max_weight', 'min_expectancy', 'max_expectancy',\n",
    "    'grooming_frequency_value', 'shedding_value', 'energy_level_value',\n",
    "    'trainability_value', 'demeanor_value'\n",
    "]\n",
    "\n",
    "# # Trim the DataFrame to only those columns\n",
    "# df = df[features_to_keep].copy()\n",
    "\n",
    "# #EDIT: Only training on models with features to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbf3fe3-aead-421f-bb34-51dd6257a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See all columns\n",
    "all_columns = df.columns.tolist()\n",
    "print(all_columns)\n",
    "print(\"Total columns:\", len(all_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d2e39-66c5-4695-ab0e-b4da16c3cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-encoding Adoption\n",
    "df['outcome_type_harmonized_grouped'] = (df['outcome_type_harmonized_grouped'] == 'adopted').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4a4ad-22a3-4292-a34e-6cfac466f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Copy original DataFrame\n",
    "encoded_df = df.copy()\n",
    "\n",
    "# Specific columns we want to encode\n",
    "columns_to_encode = [\n",
    "    'animal_type', 'primary_breed_harmonized', 'primary_color_harmonized',\n",
    "    'sex', 'intake_type_harmonized',\n",
    "    'Is_returned', 'has_name', 'is_mix'\n",
    "]\n",
    "\n",
    "# Dictionary to store label encoders\n",
    "le_dict = {}\n",
    "\n",
    "# Apply label encoding to specified columns, save in new columns\n",
    "for col in columns_to_encode:\n",
    "    le = LabelEncoder()\n",
    "    encoded_col_name = f\"Encoded-{col}\"\n",
    "    encoded_df[encoded_col_name] = le.fit_transform(encoded_df[col].astype(str))\n",
    "    le_dict[col] = le\n",
    "\n",
    "# Fill missing age_months with median\n",
    "median_age = encoded_df['age_months'].median()\n",
    "encoded_df['age_months'] = encoded_df['age_months'].fillna(median_age)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e849c5-d7ab-437c-b856-088665e61c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e905329e-2f71-4f44-949e-283888858bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761c76db-aa3e-43c5-b350-3284a5660e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Encoding locally JIC\n",
    "import pickle\n",
    "\n",
    "with open(\"label_encoders.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbb00d8-919a-475e-b37b-481b1e34c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting for training\n",
    "\n",
    "df_train = encoded_df[encoded_df['split'] == 'train']\n",
    "df_test = encoded_df[encoded_df['split'] == 'test']\n",
    "df_validate = encoded_df[encoded_df['split'] == 'validate']\n",
    "# Save each to CSV (no index)\n",
    "df_train.to_csv(\"train.csv\", index=False)\n",
    "df_test.to_csv(\"test.csv\", index=False)\n",
    "df_validate.to_csv(\"validate.csv\", index=False)\n",
    "\n",
    "# Output sizes\n",
    "print(\"Train rows:\", len(df_train))\n",
    "print(\"Test rows:\", len(df_test))\n",
    "print(\"Validate rows:\", len(df_validate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab85514-3b2a-40d7-ad0a-650580271874",
   "metadata": {},
   "source": [
    "## Model Training our XGBoost so we can Test only on those with proba >0.5. Locally-only need to do once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac44460-db9b-4916-a798-6b8b37c68fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRain Model so we can get a prediction, if our predictions are similar to endpoitn testing in other files will use this to subset on\n",
    "#adoption prediction, then do XGBoost Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4452a1-ac35-4c3e-8b10-c9c536c92fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running AMT to see if can improve Test performance\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "feature_columns = [\n",
    "'Encoded-animal_type', \n",
    "    'Encoded-primary_breed_harmonized', \n",
    "    'Encoded-primary_color_harmonized', \n",
    "    'Encoded-sex', \n",
    "    'Encoded-intake_type_harmonized', \n",
    "    'Encoded-Is_returned', \n",
    "    'Encoded-has_name', \n",
    "    'Encoded-is_mix',\n",
    "    'age_months',    \n",
    "    'Num_returned', \n",
    "    'stay_length_days', #We comment this out in our prediction for los but not here\n",
    "    'min_height', \n",
    "    'max_height',\n",
    "    'min_weight', \n",
    "    'max_weight', \n",
    "    'min_expectancy', \n",
    "    'max_expectancy',\n",
    "    'grooming_frequency_value', \n",
    "    'shedding_value', \n",
    "    'energy_level_value',\n",
    "    'trainability_value', \n",
    "    'demeanor_value'\n",
    "]\n",
    "\n",
    "\n",
    "# prepare datasets using only selected features\n",
    "X_train = df_train[feature_columns]\n",
    "y_train = df_train['outcome_type_harmonized_grouped']\n",
    "\n",
    "X_val = df_validate[feature_columns]\n",
    "y_val = df_validate['outcome_type_harmonized_grouped']\n",
    "\n",
    "X_test = df_test[feature_columns]\n",
    "y_test = df_test['outcome_type_harmonized_grouped']\n",
    "\n",
    "# Combine train and val for GridSearchCV\n",
    "X_trainval = pd.concat([X_train, X_val])\n",
    "y_trainval = pd.concat([y_train, y_val])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'gamma': [0, 2, 4],\n",
    "    'min_child_weight': [1, 4, 6],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'n_estimators': [50, 100],\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "xgb_base = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Grid search with 3-fold CV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_trainval, y_trainval)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_performance(X, y_true, dataset_name):\n",
    "    y_pred = best_model.predict(X)\n",
    "    print(f\"\\n{dataset_name} Set Performance:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f\"{dataset_name} Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Run evaluations\n",
    "evaluate_performance(X_train, y_train, \"Training\")\n",
    "evaluate_performance(X_val, y_val, \"Validation\")\n",
    "evaluate_performance(X_test, y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66347ec2-3a3d-4290-a648-1d8c1332c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outputting Best Parameters, saving model \n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)\n",
    "\n",
    "best_model.save_model(\"best_xgb_model_local.json\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad8250f-54f8-4f99-87f1-a5a820946397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We get exact same parameters so good to rain on this new data set for our length of stay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9002230f-ef91-4046-9dc8-f8e9ec0029f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we run whole encoded data set through to add our prediction and probability \n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define feature cols\n",
    "feature_columns_los = [\n",
    "    'Encoded-animal_type', \n",
    "    'Encoded-primary_breed_harmonized', \n",
    "    'Encoded-primary_color_harmonized', \n",
    "    'Encoded-sex', \n",
    "    'Encoded-intake_type_harmonized', \n",
    "    'Encoded-Is_returned', \n",
    "    'Encoded-has_name', \n",
    "    'Encoded-is_mix',\n",
    "    'age_months',    \n",
    "    'Num_returned', \n",
    "    'stay_length_days', #We comment this out in our prediction for LOS\n",
    "    'min_height', \n",
    "    'max_height',\n",
    "    'min_weight', \n",
    "    'max_weight', \n",
    "    'min_expectancy', \n",
    "    'max_expectancy',\n",
    "    'grooming_frequency_value', \n",
    "    'shedding_value', \n",
    "    'energy_level_value',\n",
    "    'trainability_value', \n",
    "    'demeanor_value'\n",
    "]\n",
    "\n",
    "# Prepare features from encoded_df\n",
    "X_encoded = encoded_df[feature_columns_los]\n",
    "\n",
    "# Check if ground truth labels exist\n",
    "if 'outcome_type_harmonized_grouped' in encoded_df.columns:\n",
    "    y_encoded = encoded_df['outcome_type_harmonized_grouped']\n",
    "else:\n",
    "    y_encoded = None\n",
    "\n",
    "# Run prediction and predicted probabilities\n",
    "y_pred = best_model.predict(X_encoded)\n",
    "y_proba = best_model.predict_proba(X_encoded)[:, 1]  # Probability of positive class (adopted)\n",
    "\n",
    "# Add predictions and probabilities to df\n",
    "encoded_df['predicted_label'] = y_pred\n",
    "encoded_df['predicted_proba'] = y_proba\n",
    "\n",
    "# Evaluate Performance\n",
    "if y_encoded is not None:\n",
    "    print(\"Classification Report on encoded_df:\")\n",
    "    print(classification_report(y_encoded, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_encoded, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix on encoded_df\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No ground truth labels found; predictions added to DataFrame.\")\n",
    "\n",
    "\n",
    "print(encoded_df[['predicted_label', 'predicted_proba']].head())\n",
    "\n",
    "encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dbe2b8-b444-4195-b5b2-6a21cd590d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See all columns\n",
    "all_columns = encoded_df.columns.tolist()\n",
    "print(all_columns)\n",
    "print(\"Total columns:\", len(all_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73870b1e-0257-4f12-8f22-74d7c31ce446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next We see how many length of stay is NAN/Null and decide if we drop\n",
    "null_count = encoded_df['stay_length_days'].isnull().sum()\n",
    "print(f\"Number of null values in 'stay_length_days': {null_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0432da0-399f-4eb2-8288-86bd23dc1b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given small number we'll take average\n",
    "\n",
    "mean_stay_length = encoded_df['stay_length_days'].mean()\n",
    "\n",
    "# Fill NaNs\n",
    "encoded_df['stay_length_days'] = encoded_df['stay_length_days'].fillna(mean_stay_length)\n",
    "\n",
    "print(f\"Filled NaNs in 'stay_length_days' with mean value: {mean_stay_length:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab4377a-c062-4a6b-ae64-8a48b4874365",
   "metadata": {},
   "source": [
    "## TRaining for Length of Stay as Numeric using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030fd54b-ba32-4032-b41d-d6108983d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll train locally to find our best fit but first we split again for train/test/validate\n",
    "# Splitting for training\n",
    "\n",
    "los_predicted_df_train = encoded_df[encoded_df['split'] == 'train']\n",
    "los_predicted_df_test = encoded_df[(encoded_df['split'] == 'test') & (encoded_df['predicted_proba'] >= 0.5)]\n",
    "los_predicted_df_validate = encoded_df[encoded_df['split'] == 'validate']\n",
    "\n",
    "los_predicted_df_train.to_csv(\"los_train.csv\", index=False)\n",
    "los_predicted_df_test.to_csv(\"los_test.csv\", index=False)\n",
    "los_predicted_df_validate.to_csv(\"los_validate.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Output sizes\n",
    "print(\"Train rows:\", len(los_predicted_df_train))\n",
    "print(\"Test rows:\", len(los_predicted_df_test))\n",
    "print(\"Validate rows:\", len(los_predicted_df_validate))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306b107-ea2e-4b5f-bfd6-91699a149131",
   "metadata": {},
   "outputs": [],
   "source": [
    "los_predicted_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9baae25-0237-48b1-a296-ee9dac93b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we train locally using AMT to find our best model fit and ensure our Test is only on animals\n",
    "#that are predicted adoption >=50%\n",
    "\n",
    "#Running AMT to see if can improve Test performance\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error #Switched to numeric rather than categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "feature_columns_los = [\n",
    "    'Encoded-animal_type', \n",
    "    'Encoded-primary_breed_harmonized', \n",
    "    'Encoded-primary_color_harmonized', \n",
    "    'Encoded-sex', \n",
    "    'Encoded-intake_type_harmonized', \n",
    "    'Encoded-Is_returned', \n",
    "    'Encoded-has_name', \n",
    "    'Encoded-is_mix',\n",
    "    'age_months',    \n",
    "    'Num_returned', \n",
    "    #'stay_length_days', #edit: We dropped this out for training as is now our predicted variable\n",
    "    'min_height', \n",
    "    'max_height',\n",
    "    'min_weight', \n",
    "    'max_weight', \n",
    "    'min_expectancy', \n",
    "    'max_expectancy',\n",
    "    'grooming_frequency_value', \n",
    "    'shedding_value', \n",
    "    'energy_level_value',\n",
    "    'trainability_value', \n",
    "    'demeanor_value'\n",
    "]\n",
    "\n",
    "# Prepare datasets using selected features\n",
    "X_train = los_predicted_df_train[feature_columns_los]\n",
    "y_train = los_predicted_df_train['stay_length_days']  \n",
    "\n",
    "X_val = los_predicted_df_validate[feature_columns_los]\n",
    "y_val = los_predicted_df_validate['stay_length_days']\n",
    "\n",
    "X_test = los_predicted_df_test[feature_columns_los]\n",
    "y_test = los_predicted_df_test['stay_length_days']\n",
    "\n",
    "# Integrate as gridsearch manages train/val\n",
    "X_trainval = pd.concat([X_train, X_val])\n",
    "y_trainval = pd.concat([y_train, y_val])\n",
    "\n",
    "# Define grid, kept as above for consistency\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'gamma': [0, 2, 4],\n",
    "    'min_child_weight': [1, 4, 6],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'n_estimators': [50, 100],\n",
    "}\n",
    "\n",
    "# Initialize regression model\n",
    "xgb_base = XGBRegressor(\n",
    "    objective='reg:squarederror',#Switched to numeric rather than categorical\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Grid search with 3-fold CV, scoring R^2\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_trainval, y_trainval)\n",
    "best_model_los = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best parameters fond:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluation function for regression\n",
    "def evaluate_regression_performance(X, y_true, dataset_name):\n",
    "    y_pred = best_model_los.predict(X)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    print(f\"\\n{dataset_name} Set Performance:\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Run evaluations\n",
    "evaluate_regression_performance(X_train, y_train, \"Training\")\n",
    "evaluate_regression_performance(X_val, y_val, \"Validation\")\n",
    "evaluate_regression_performance(X_test, y_test, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5748de3-f6e8-45e0-946c-780e800d7fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outputting Best Parameters, saving model \n",
    "\n",
    "best_params_los = grid_search.best_params_\n",
    "print(best_params_los)\n",
    "\n",
    "best_model.save_model(\"best_xgb_model_los_local.json\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9388ee6e-9a26-4647-95ea-8d70fe094761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show scatterplot of Test\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = best_model_los.predict(X_test)\n",
    "\n",
    "# Filter where both actual and predicted < 100\n",
    "mask = (y_test < 100) & (y_test_pred < 100)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test[mask], y_test_pred[mask], alpha=0.5)\n",
    "plt.plot([0, 100], [0, 100], 'r--', linewidth=2)  # line\n",
    "plt.xlabel(\"Actual stay_length_days (<100)\")\n",
    "plt.ylabel(\"Predicted stay_length_days (<100)\")\n",
    "plt.title(\"Actual vs Predicted Stay Length (Test Set, <100 days)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96918e56-a1ee-4bbb-98ec-cc3a410bb868",
   "metadata": {},
   "source": [
    "## Training for XGBoost Endpoint Length of Stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a80cb9-6620-46da-af36-95fcec045a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seeing our DFs\n",
    "los_predicted_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f8a218-ccd3-4041-8e1f-c3e47930fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "los_predicted_df_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba7321-e815-4d24-be03-9aa792122aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "los_predicted_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f628c75-1888-4c13-ba82-ea967d91fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remembering our columns so we can pass the right features to XGboost\n",
    "\n",
    "all_columns = los_predicted_df_test.columns.tolist()\n",
    "print(all_columns)\n",
    "print(\"Total columns for los_predicted structure:\", len(all_columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3478289-e144-4cb4-ae5f-075820c1da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Prep from above work to S3 so sagemaker can access\n",
    "\n",
    "#setup\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import pandas as pd\n",
    "\n",
    "# sagemaker seetup\n",
    "session = sagemaker.Session()\n",
    "role = \"arn:aws:iam::917456409349:role/Sagemaker_Execution_Role\"\n",
    "\n",
    "bucket = \"xgb-los-multi\"\n",
    "prefix = \"xgb-los-model-code\"\n",
    "data_prefix = f\"{prefix}/data\"\n",
    "\n",
    "#data prep\n",
    "feature_columns = [\n",
    "    'Encoded-animal_type',\n",
    "    'Encoded-primary_breed_harmonized',\n",
    "    'Encoded-primary_color_harmonized',\n",
    "    'Encoded-sex',\n",
    "    'Encoded-intake_type_harmonized',\n",
    "    'Encoded-Is_returned',\n",
    "    'Encoded-has_name',\n",
    "    'Encoded-is_mix',\n",
    "    'age_months',\n",
    "    'Num_returned',\n",
    "    'min_height',\n",
    "    'max_height',\n",
    "    'min_weight',\n",
    "    'max_weight',\n",
    "    'min_expectancy',\n",
    "    'max_expectancy',\n",
    "    'grooming_frequency_value',\n",
    "    'shedding_value',\n",
    "    'energy_level_value',\n",
    "    'trainability_value',\n",
    "    'demeanor_value'\n",
    "]\n",
    "target_column = \"stay_length_days\"\n",
    "\n",
    "#dataframe prep function\n",
    "def prepare_for_sagemaker(df):\n",
    "    df = df[feature_columns + [target_column]]\n",
    "    return df[[target_column] + feature_columns]  # move target to front for sagemaker\n",
    "\n",
    "# Prepare datasets\n",
    "train_df = prepare_for_sagemaker(los_predicted_df_train)\n",
    "val_df = prepare_for_sagemaker(los_predicted_df_validate)\n",
    "test_df = prepare_for_sagemaker(los_predicted_df_test)\n",
    "\n",
    "# Save as CSV (no header/index)\n",
    "train_df.to_csv(\"train.csv\", header=False, index=False)\n",
    "val_df.to_csv(\"validation.csv\", header=False, index=False)\n",
    "test_df.to_csv(\"test.csv\", header=False, index=False)\n",
    "\n",
    "#S3 Upload for sagemaker to access\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.upload_file(\"train.csv\", bucket, f\"{data_prefix}/train.csv\")\n",
    "s3.upload_file(\"validation.csv\", bucket, f\"{data_prefix}/validation.csv\")\n",
    "s3.upload_file(\"test.csv\", bucket, f\"{data_prefix}/test.csv\")\n",
    "\n",
    "print(\" Uploaded to:\")\n",
    "print(f\"s3://{bucket}/{data_prefix}/train.csv\")\n",
    "print(f\"s3://{bucket}/{data_prefix}/validation.csv\")\n",
    "print(f\"s3://{bucket}/{data_prefix}/test.csv\")\n",
    "\n",
    "train_s3_path = f\"s3://{bucket}/{data_prefix}/train.csv\"\n",
    "val_s3_path = f\"s3://{bucket}/{data_prefix}/validation.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37570e77-539e-4277-8a9f-b717ed85fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we access S3 to run our endpoint and deployment training\n",
    "\n",
    "#container image for XGBoost\n",
    "\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", session.boto_region_name, version=\"1.5-1\")\n",
    "\n",
    "# Create estimator\n",
    "xgb_estimator_los = Estimator(\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size=5,\n",
    "    max_run=3600,\n",
    "    output_path=f\"s3://{bucket}/{prefix}/output\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Set hyperparameters fpr training\n",
    "xgb_estimator_los.set_hyperparameters(\n",
    "    objective=\"reg:squarederror\",\n",
    "    gamma=0,\n",
    "    eta=0.05,               # learning_rate\n",
    "    max_depth=5,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.6,\n",
    "    num_round=50            # n_estimators equivalent\n",
    ")\n",
    "\n",
    "print(\" Starting training job...\")\n",
    "xgb_estimator_los.fit({\n",
    "    \"train\": sagemaker.inputs.TrainingInput(train_s3_path, content_type=\"text/csv\"),\n",
    "    \"validation\": sagemaker.inputs.TrainingInput(val_s3_path, content_type=\"text/csv\")\n",
    "})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82ca6f6-9b9a-4f57-8590-f783b2860a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last, we deploy\n",
    "\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "\n",
    "print(\" Deploying endpoint...\")\n",
    "xgb_predictor_los = xgb_estimator_los.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    endpoint_name=\"xgb-los-endpoint\"\n",
    ")\n",
    "\n",
    "# Configure predictor\n",
    "xgb_predictor_los.serializer = CSVSerializer()\n",
    "print (\"endpoitn is live:\",xgb_predictor_los.endpoint_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07fcbac-247f-47bb-a6e6-771b565bdf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we test the endpoint using test.csv as test_df\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "#Prepare features\n",
    "target_column = \"stay_length_days\"\n",
    "feature_columns = [\n",
    "    'Encoded-animal_type',\n",
    "    'Encoded-primary_breed_harmonized',\n",
    "    'Encoded-primary_color_harmonized',\n",
    "    'Encoded-sex',\n",
    "    'Encoded-intake_type_harmonized',\n",
    "    'Encoded-Is_returned',\n",
    "    'Encoded-has_name',\n",
    "    'Encoded-is_mix',\n",
    "    'age_months',\n",
    "    'Num_returned',\n",
    "    'min_height',\n",
    "    'max_height',\n",
    "    'min_weight',\n",
    "    'max_weight',\n",
    "    'min_expectancy',\n",
    "    'max_expectancy',\n",
    "    'grooming_frequency_value',\n",
    "    'shedding_value',\n",
    "    'energy_level_value',\n",
    "    'trainability_value',\n",
    "    'demeanor_value'\n",
    "]\n",
    "\n",
    "X_test = test_df[feature_columns].fillna(0)\n",
    "y_test = test_df[target_column].values\n",
    "\n",
    "print(f\" Using {len(X_test)} rows and {len(feature_columns)} features for inference\")\n",
    "\n",
    "#Send batched endpoints\n",
    "xgb_predictor_los.serializer = CSVSerializer()\n",
    "predictions = []\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(0, X_test.shape[0], batch_size):\n",
    "    batch = X_test.iloc[i:i+batch_size]\n",
    "    payload = \"\\n\".join([\",\".join(map(str, row)) for row in batch.to_numpy()])\n",
    "    \n",
    "    if i == 0:\n",
    "        print(\"Sample payload being sent:\", payload.split(\"\\n\")[0])\n",
    "    \n",
    "    response = xgb_predictor_los.predict(payload)\n",
    "    decoded = response.decode(\"utf-8\").strip()\n",
    "    \n",
    "    if decoded:\n",
    "        predictions.extend([float(x) for x in decoded.split(\"\\n\")])\n",
    "    else:\n",
    "        print(f\" Empty response for batch {i}-{i+batch_size}\")\n",
    "\n",
    "print(f\" Predictions received: {len(predictions)} rows\")\n",
    "\n",
    "#combine and evaluate\n",
    "results_df = pd.DataFrame({\n",
    "    \"Actual\": y_test,\n",
    "    \"Predicted\": predictions\n",
    "})\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(results_df[\"Actual\"], results_df[\"Predicted\"]))\n",
    "r2 = r2_score(results_df[\"Actual\"], results_df[\"Predicted\"])\n",
    "print(f\" RMSE: {rmse:.4f}\")\n",
    "print(f\" R²: {r2:.4f}\")\n",
    "\n",
    "#Scatter plot\n",
    "mask = (results_df[\"Actual\"] < 100) & (results_df[\"Predicted\"] < 100)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(results_df[\"Actual\"][mask], results_df[\"Predicted\"][mask], alpha=0.5)\n",
    "plt.plot([0, 100], [0, 100], 'r--', linewidth=2)\n",
    "plt.xlabel(\"Actual stay_length_days (<100)\")\n",
    "plt.ylabel(\"Predicted stay_length_days (<100)\")\n",
    "plt.title(\"Actual vs Predicted Stay Length (Endpoint Inference, <100 days)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10964223-06d9-452a-b5c7-c7509f02b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We get very similar results, so model is operating as expected locally and via endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551191a5-b459-4d0e-bb70-f6049ecd9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f233545-a456-494d-a234-7cb377d7bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
