{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNbfHiwkxoc9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shap\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from itertools import cycle\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv(\"df_cat_dog_harmonized.csv\")"
      ],
      "metadata": {
        "id": "6P6gE6seydnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocess - Created a \"regroupped\" outcome_type by remapping the outcome_type\n",
        "def map_outcome_group(value):\n",
        "    adopted = {\"adoption\"}\n",
        "    non_adopted = {\n",
        "        \"rescue\", \"foster\", \"return to owner\",\n",
        "        \"foster to adopt\", \"return to rescue\", \"rtf\"\n",
        "    }\n",
        "\n",
        "    value = str(value).strip().lower()\n",
        "\n",
        "    if value in adopted:\n",
        "        return \"adopted\"\n",
        "    elif value in non_adopted:\n",
        "        return \"non-adopted\"\n",
        "    else:\n",
        "        return \"other\"\n",
        "\n",
        "df[\"outcome_type_harmonized_regrouped\"] = df[\"outcome_type_harmonized\"].apply(map_outcome_group)"
      ],
      "metadata": {
        "id": "KvGpwl79ygEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Preprocessing ---\n",
        "def encode_categorical(df, categorical_cols):\n",
        "    encoders = {}\n",
        "    for col in categorical_cols:\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col].astype(str))\n",
        "        encoders[col] = le\n",
        "    return df, encoders\n",
        "\n",
        "def transform_categorical(df, categorical_cols, encoders):\n",
        "    for col in categorical_cols:\n",
        "        if col in encoders:\n",
        "            le = encoders[col]\n",
        "            df[col] = df[col].map(lambda x: le.transform([x])[0] if x in le.classes_ else -1)\n",
        "    return df\n",
        "\n",
        "def scale_numerical(df, numerical_cols, scaler=None):\n",
        "    df = df.copy()\n",
        "    df[numerical_cols] = df[numerical_cols].astype(np.float32)\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        df.loc[:, numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "    else:\n",
        "        df.loc[:, numerical_cols] = scaler.transform(df[numerical_cols])\n",
        "    return df, scaler\n",
        "\n",
        "def encode_booleans(df, boolean_cols):\n",
        "    for col in boolean_cols:\n",
        "        df[col] = df[col].map({'yes': 1, 'no': 0}).fillna(0).astype(int)\n",
        "    return df\n",
        "\n",
        "def train_xgb_classifier(X_train, y_train, **kwargs):\n",
        "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', **kwargs)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def train_xgb_regressor(X_train, y_train, **kwargs):\n",
        "    model = XGBRegressor(**kwargs)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def evaluate_regression(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
        "\n",
        "def evaluate_classification(y_true, y_pred, target_names=['non-adopted', 'adopted']):\n",
        "    report = classification_report(y_true, y_pred, target_names=target_names, output_dict=True)\n",
        "    return pd.DataFrame(report).transpose()\n",
        "\n",
        "def train_logistic_classification(X_train, y_train, **kwargs):\n",
        "    model = LogisticRegression(max_iter=1000, **kwargs)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def train_rf_classifier(X_train, y_train, **kwargs):\n",
        "    model = RandomForestClassifier(**kwargs)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def train_rf_regressor(X_train, y_train, **kwargs):\n",
        "    model = RandomForestRegressor(**kwargs)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- Step 1: Preprocessing\n",
        "boolean_cols = ['Is_returned', 'is_mix']\n",
        "categorical_cols = [\n",
        "    'animal_type', 'primary_breed_harmonized', 'primary_color_harmonized',\n",
        "    'sex', 'intake_type_harmonized', 'has_name',\n",
        "] + boolean_cols\n",
        "\n",
        "numerical_cols = [\n",
        "    'Num_returned', 'age_months', 'stay_length_days', 'min_height', 'max_height',\n",
        "    'min_weight', 'max_weight', 'min_expectancy', 'max_expectancy',\n",
        "    'grooming_frequency_value', 'shedding_value', 'energy_level_value',\n",
        "    'trainability_value', 'demeanor_value'\n",
        "]\n",
        "\n",
        "target_col_cls = 'adopt_label'\n",
        "features_cls = categorical_cols + numerical_cols\n",
        "\n",
        "# --- Data Preparation ---\n",
        "# Encode booleans and categorical\n",
        "df = encode_booleans(df, boolean_cols)\n",
        "df, encoders = encode_categorical(df, categorical_cols)\n",
        "\n",
        "# Filter and split for classification\n",
        "df[target_col_cls] = df['outcome_type_harmonized_grouped'].map({'adopted': 1, 'non-adopted': 0})\n",
        "df = df.dropna(subset=features_cls + [target_col_cls])\n",
        "\n",
        "train_df = df[df['outcome_year'].between(2018, 2023)]\n",
        "val_df = df[df['outcome_year'] == 2024]\n",
        "test_df = df[df['outcome_year'] == 2025]\n",
        "\n",
        "# Scale numericals\n",
        "train_df, scaler = scale_numerical(train_df, numerical_cols)\n",
        "val_df, _ = scale_numerical(val_df, numerical_cols, scaler)\n",
        "test_df, _ = scale_numerical(test_df, numerical_cols, scaler)\n",
        "\n",
        "# --- Step 2a: Train classifier through XGBoost and Random Forest---\n",
        "X_train_cls = train_df[features_cls]\n",
        "y_train_cls = train_df[target_col_cls]\n",
        "X_val_cls = val_df[features_cls]\n",
        "y_val_cls = val_df[target_col_cls]\n",
        "X_test_cls = test_df[features_cls]\n",
        "y_test_cls = test_df[target_col_cls]\n",
        "\n",
        "# --- XGBoost Classifer\n",
        "clf_model = train_xgb_classifier(X_train_cls, y_train_cls, max_depth=6, n_estimators=150, learning_rate=0.05)\n",
        "\n",
        "# --- Logistic Classifer ---\n",
        "logclf_model = train_logistic_classification(X_train_cls, y_train_cls, solver='lbfgs', C=1.0)\n",
        "\n",
        "# --- Step 3a: Evaluate adopt score prediction ---\n",
        "# --- Evaluate XGboost Classifier results\n",
        "y_pred_val_cls = clf_model.predict(X_val_cls)\n",
        "print(\"\\n Val Classification Report for adoption score (XGBoost):\")\n",
        "print(evaluate_classification(y_val_cls, y_pred_val_cls))\n",
        "\n",
        "y_pred_test_cls = clf_model.predict(X_test_cls)\n",
        "print(\"\\n Test Classification Report for adoption score (XGBoost):\")\n",
        "print(evaluate_classification(y_test_cls, y_pred_test_cls))\n",
        "\n",
        "# --- Evaluate Logistic Classifier results ---\n",
        "y_pred_val_logclf = logclf_model.predict(X_val_cls)\n",
        "print(\"\\n Validation Classification Report for adoption score (Logistic):\")\n",
        "print(evaluate_classification(y_val_cls, y_pred_val_logclf))\n",
        "\n",
        "y_pred_test_logclf = logclf_model.predict(X_test_cls)\n",
        "print(\"\\n Test Classification Report for adoption score (Logistic):\")\n",
        "print(evaluate_classification(y_test_cls, y_pred_test_logclf))\n",
        "\n",
        "# Predict adoption probability and scale to 0â€“100\n",
        "y_pred_proba = clf_model.predict_proba(X_test_cls)[:, 1]\n",
        "logclf_proba = logclf_model.predict_proba(X_test_cls)[:, 1]\n",
        "\n",
        "test_df = test_df.copy()\n",
        "test_df[\"xgb_predicted_adopt_score\"] = np.round(y_pred_proba * 100, 1).astype(np.float32)\n",
        "test_df[\"log_predicted_adopt_score\"] = np.round(logclf_proba * 100, 1).astype(np.float32)\n",
        "test_df[\"is_adopted_prediction\"] = test_df[\"xgb_predicted_adopt_score\"] >= 50\n",
        "combined_df = test_df.copy()\n",
        "\n",
        "# --- Step 2b: Train regressor for stay_length_days using XGBoost and Random Forest---\n",
        "features_reg = [col for col in features_cls if col != 'stay_length_days']\n",
        "df_reg = df.dropna(subset=features_reg + ['stay_length_days'])\n",
        "\n",
        "train_df_reg = df_reg[df_reg['outcome_year'].between(2018, 2023)]\n",
        "val_df_reg = df_reg[df_reg['outcome_year'] == 2024]\n",
        "test_df_reg = df_reg[df_reg['outcome_year'] == 2025]\n",
        "\n",
        "X_train_reg = train_df_reg[features_reg]\n",
        "y_train_reg = train_df_reg['stay_length_days']\n",
        "X_val_reg = val_df_reg[features_reg]\n",
        "y_val_reg = val_df_reg['stay_length_days']\n",
        "X_test_reg = test_df_reg[features_reg]\n",
        "y_test_reg = test_df_reg['stay_length_days']\n",
        "\n",
        "# XGBoost\n",
        "reg_model = train_xgb_regressor(X_train_reg, y_train_reg, max_depth=6, n_estimators=100, learning_rate=0.05)\n",
        "# Random Forest\n",
        "reg_model_rf = train_rf_regressor(X_train_reg, y_train_reg, n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "# --- Step 3b: Evaluate regression ---\n",
        "# --- XGBoost regression evaluation ---\n",
        "y_pred_val_reg = reg_model.predict(X_val_reg)\n",
        "print(\"\\n Val Regression Metrics for stay_length (XGBoost):\")\n",
        "print(evaluate_regression(y_val_reg, y_pred_val_reg))\n",
        "\n",
        "y_pred_test_reg = reg_model.predict(X_test_reg)\n",
        "print(\"\\n Test Regression Metrics for stay_length (XGBoost):\")\n",
        "print(evaluate_regression(y_test_reg, y_pred_test_reg))\n",
        "\n",
        "test_df_reg = test_df_reg.copy()\n",
        "test_df_reg[\"predicted_stay_length_days\"] = np.round(y_pred_test_reg, 1).astype(np.float32)\n",
        "\n",
        "# --- Random Forest regression evaluation ---\n",
        "y_pred_val_reg_rf = reg_model_rf.predict(X_val_reg)\n",
        "print(\"\\n Val Regression Metrics for stay_length (Random Forest):\")\n",
        "print(evaluate_regression(y_val_reg, y_pred_val_reg_rf))\n",
        "\n",
        "y_pred_test_reg_rf = reg_model_rf.predict(X_test_reg)\n",
        "print(\"\\n Test Regression Metrics for stay_length (Random Forest):\")\n",
        "print(evaluate_regression(y_test_reg, y_pred_test_reg_rf))\n",
        "\n",
        "# --- Step 3d: Save predictions from one model (choose XGBoost here) ---\n",
        "test_df_reg = test_df_reg.copy()\n",
        "test_df_reg[\"predicted_stay_length_days_xgb\"] = np.round(y_pred_test_reg, 1).astype(np.float32)\n",
        "test_df_reg[\"predicted_stay_length_days_rf\"] = np.round(y_pred_test_reg_rf, 1).astype(np.float32)\n",
        "# Filter to only those that exist in combined_df for safety (adopted predictions)\n",
        "test_df_reg = test_df_reg[test_df_reg.index.isin(combined_df.index)]\n",
        "\n",
        "# --- Merge predicted stay lengths for adopted animals ---\n",
        "#stay_length_cols = [\"predicted_stay_length_days_xgb\", \"predicted_stay_length_days_rf\"]\n",
        "#combined_df = combined_df.join(test_df_reg[stay_length_cols], how=\"left\")\n",
        "\n",
        "# Mask for adopted predictions\n",
        "#adopted_mask = combined_df[\"is_adopted_prediction\"] == True\n",
        "\n",
        "# Apply stay length predictions only to adopted rows, set others to NaN\n",
        "mask = combined_df[\"is_adopted_prediction\"] == True\n",
        "test_df_reg = test_df_reg[test_df_reg.index.isin(combined_df[mask].index)]\n",
        "combined_df = combined_df.join(\n",
        "    test_df_reg[[\"predicted_stay_length_days_xgb\", \"predicted_stay_length_days_rf\"]],\n",
        "    how=\"left\"\n",
        ")\n",
        "# --- Step 4: Non-Adopted Outcome Subtype Classification using Random Forest and XGBoost ---\n",
        "# Create `non_adopted_label` from harmonized subtype mappings\n",
        "non_adopted_subtypes = {\n",
        "    \"rescue\": \"rescue\",\n",
        "    \"foster\": \"foster\",\n",
        "    \"return to owner\": \"return_to_owner\",\n",
        "    \"foster to adopt\": \"foster\",\n",
        "    \"return to rescue\": \"rescue\",\n",
        "    \"rtf\": \"foster\"\n",
        "}\n",
        "df[\"non_adopted_label\"] = df[\"outcome_type_harmonized\"].map(non_adopted_subtypes)\n",
        "\n",
        "# Filter non-adopted and drop incomplete rows\n",
        "df_non_adopted = df[df['outcome_type_harmonized_regrouped'] == 'non-adopted'].copy()\n",
        "df_non_adopted = df_non_adopted.dropna(subset=categorical_cols + numerical_cols + ['non_adopted_label'])\n",
        "\n",
        "# Prepare splits\n",
        "train_df_reg_na = df_non_adopted[df_non_adopted['outcome_year'].between(2018, 2023)]\n",
        "val_df_reg_na = df_non_adopted[df_non_adopted['outcome_year'] == 2024]\n",
        "#test_df_reg_na = df_non_adopted[df_non_adopted['outcome_year'] == 2025]\n",
        "\n",
        "# --- Filter only non-adopted test samples with XGBoost adoption score < 50 ---\n",
        "test_df_with_scores = test_df.copy()\n",
        "test_df_with_scores = test_df_with_scores[test_df_with_scores[\"xgb_predicted_adopt_score\"] < 50]\n",
        "\n",
        "# Join with df_non_adopted to get outcome label\n",
        "test_df_reg_na = df_non_adopted[df_non_adopted['outcome_year'] == 2025]\n",
        "test_df_reg_na = test_df_reg_na[test_df_reg_na.index.isin(test_df_with_scores.index)]\n",
        "\n",
        "\n",
        "X_train_na = train_df_reg_na[categorical_cols + numerical_cols]\n",
        "y_train_na = train_df_reg_na[\"non_adopted_label\"]\n",
        "X_val_na = val_df_reg_na[categorical_cols + numerical_cols]\n",
        "y_val_na = val_df_reg_na[\"non_adopted_label\"]\n",
        "X_test_na = test_df_reg_na[categorical_cols + numerical_cols]\n",
        "y_test_na = test_df_reg_na[\"non_adopted_label\"]\n",
        "\n",
        "# --- Encode labels for non-adopted subtypes ---\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(pd.concat([y_train_na, y_val_na, y_test_na], axis=0))\n",
        "\n",
        "y_train_encoded = label_encoder.transform(y_train_na)\n",
        "y_val_encoded = label_encoder.transform(y_val_na)\n",
        "y_test_encoded = label_encoder.transform(y_test_na)\n",
        "\n",
        "# --- Random Forest Classifier for Non-adopted Subtypes ---\n",
        "rf_na_model = train_rf_classifier(X_train_na, y_train_na, n_estimators=100, max_depth=10, random_state=42, class_weight=\"balanced\")\n",
        "# Validation/Test performance\n",
        "y_pred_val_na = rf_na_model.predict(X_val_na)\n",
        "y_pred_test_na = rf_na_model.predict(X_test_na)\n",
        "\n",
        "print(\"\\n Val Set Classification Report (Random Forest):\")\n",
        "print(classification_report(y_val_na, y_pred_val_na))\n",
        "#print(f\"Accuracy (Val): {accuracy_score(y_val_na, y_pred_val_na):.4f}\")\n",
        "print(\"\\n Test Set Classification Report (Random Forest):\")\n",
        "print(classification_report(y_test_na, y_pred_test_na))\n",
        "#print(f\"Accuracy (Test): {accuracy_score(y_test_na, y_pred_test_na):.4f}\")\n",
        "\n",
        "# --- XGBoost Classifier for Non-Adopted Subtypes ---\n",
        "xgb_na_model = train_xgb_classifier(X_train_na, y_train_encoded, max_depth=10, n_estimators=100, learning_rate=0.05, random_state=42)\n",
        "y_pred_val_xgb_na = xgb_na_model.predict(X_val_na)\n",
        "y_pred_test_xgb_na = xgb_na_model.predict(X_test_na)\n",
        "y_proba_test_xgb_na = xgb_na_model.predict_proba(X_test_na)\n",
        "\n",
        "print(\"\\n Val Set Classification Report (XGBoost):\")\n",
        "print(classification_report(y_val_encoded, y_pred_val_xgb_na, target_names=label_encoder.classes_))\n",
        "print(\"\\n Test Set Classification Report (XGBoost):\")\n",
        "print(classification_report(y_test_encoded, y_pred_test_xgb_na, target_names=label_encoder.classes_))\n",
        "\n",
        "\n",
        "# --- Step 5: Combine results and SHAP for classifier ---\n",
        "#combined_df = test_df.join(test_df_reg[[\"predicted_stay_length_days\", \"predicted_stay_length_days_rf\"]])\n",
        "\n",
        "# Add predicted non-adopted subtype to test_df_reg_na\n",
        "test_df_reg_na = test_df_reg_na.copy()\n",
        "test_df_reg_na[\"rf_predicted_non_adopted_label\"] = y_pred_test_na\n",
        "test_df_reg_na[\"xgb_predicted_non_adopted_label\"] = label_encoder.inverse_transform(y_pred_test_xgb_na)\n",
        "\n",
        "# Optional: XGBoost probability scores (per class)\n",
        "for i, class_label in enumerate(label_encoder.classes_):\n",
        "    test_df_reg_na[f\"xgb_proba_{class_label}\"] = y_proba_test_xgb_na[:, i]\n",
        "\n",
        "# Merge all predictions into a single DataFrame by common indices\n",
        "combined_df = test_df.copy()\n",
        "\n",
        "# Merge predicted stay lengths\n",
        "combined_df = combined_df.join(\n",
        "    test_df_reg[[\"predicted_stay_length_days_xgb\", \"predicted_stay_length_days_rf\"]]\n",
        ")\n",
        "\n",
        "# Merge predicted non-adopted subtypes\n",
        "combined_df = combined_df.join(\n",
        "    test_df_reg_na[[\n",
        "        \"rf_predicted_non_adopted_label\",\n",
        "        \"xgb_predicted_non_adopted_label\"\n",
        "    ] + [f\"xgb_proba_{cls}\" for cls in label_encoder.classes_]],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Final combined_df now contains:\n",
        "# - Adoption scores from XGBoost and Logistic Regression\n",
        "# - Stay length predictions from XGBoost and Random Forest\n",
        "# - Subtype predictions from RF and XGBoost\n",
        "# - XGBoost probability distribution over subtype classes\n",
        "\n",
        "# Preview combined_df\n",
        "print(\"\\n Final Combined DataFrame (Sample):\")\n",
        "display(combined_df.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "xj-3z2ryyvAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df.columns"
      ],
      "metadata": {
        "id": "GTZo3_vWPcpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_columns = [\n",
        "    \"animal_id\", \"has_name\", \"animal_type\", \"primary_breed_harmonized\", \"primary_color_harmonized\",\n",
        "    \"sex\", \"intake_type_harmonized\", \"shelter\",\n",
        "    \"xgb_predicted_adopt_score\", \"log_predicted_adopt_score\", \"predicted_stay_length_days_xgb\",\n",
        "    \"xgb_predicted_non_adopted_label\", \"xgb_proba_foster\", \"xgb_proba_rescue\", \"xgb_proba_return_to_owner\",\n",
        "    \"Num_returned\", \"age_months\", \"stay_length_days\", \"is_adopted_prediction\"\n",
        "]\n",
        "\n",
        "combined_df[display_columns].head(10)\n"
      ],
      "metadata": {
        "id": "8q1rsm-nPXwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 6: Decode categorical values in combined_df ---\n",
        "categorical_to_decode = [\n",
        "    \"animal_type\", \"primary_breed_harmonized\", \"primary_color_harmonized\",\n",
        "    \"sex\", \"intake_type_harmonized\", 'has_name', 'Is_returned', 'is_mix'\n",
        "]\n",
        "\n",
        "for col in categorical_to_decode:\n",
        "    if col in encoders and col in combined_df.columns:\n",
        "        le = encoders[col]\n",
        "        # Ensure integer conversion and mask invalid values\n",
        "        valid_idx = combined_df[col].apply(lambda x: isinstance(x, (int, np.integer)) and 0 <= x < len(le.classes_))\n",
        "        decoded_vals = combined_df.loc[valid_idx, col].astype(float).astype(int)\n",
        "        combined_df.loc[valid_idx, col] = le.inverse_transform(decoded_vals)"
      ],
      "metadata": {
        "id": "ec3qIGRUTHyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 7: Decode numerical values in combined_df ---\n",
        "numerical_cols_decodable = [col for col in numerical_cols if col in combined_df.columns]\n",
        "decoded_matrix = combined_df[numerical_cols_decodable].copy()\n",
        "\n",
        "# Fill NaNs to avoid transform errors\n",
        "decoded_matrix_filled = decoded_matrix.fillna(0)\n",
        "\n",
        "# Perform inverse transform on all columns the scaler saw\n",
        "inversed_matrix = scaler.inverse_transform(decoded_matrix_filled)\n",
        "\n",
        "# Replace columns with inverse transformed values\n",
        "combined_df.loc[:, numerical_cols_decodable] = inversed_matrix\n",
        "\n",
        "# --- Round adopt scores to integers (0â€“100 scale) ---\n",
        "score_cols = [\"XGBoost_predicted_adopt_score\", \"Logistic_predicted_adopt_score\"]\n",
        "for col in score_cols:\n",
        "    if col in combined_df.columns:\n",
        "        combined_df[col] = combined_df[col].round(0).astype(\"Int64\")"
      ],
      "metadata": {
        "id": "5hZyqPhwTR8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df[display_columns].head(10)"
      ],
      "metadata": {
        "id": "UfTrysTeTZMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SHAP for Test Data ---\n",
        "explainer = shap.Explainer(clf_model, X_train_cls)\n",
        "shap_values = explainer(X_test_cls)\n",
        "\n",
        "# Add SHAP values as separate columns\n",
        "shap_array = shap_values.values\n",
        "feature_names = shap_values.feature_names\n",
        "for i, feature in enumerate(feature_names):\n",
        "    combined_df[f\"{feature}_SHAP\"] = shap_array[:, i]\n",
        "\n",
        "# --- SHAP for Train Data ---\n",
        "explainer_train = shap.Explainer(clf_model, X_train_cls)\n",
        "#shap_values_train = explainer_train(X_train_cls)\n",
        "\n",
        "# Add SHAP values to a copy of train_df\n",
        "#shap_array_train = shap_values_train.values\n",
        "#feature_names = shap_values_train.feature_names\n",
        "#for i, feature in enumerate(feature_names):\n",
        "#    combined_df[f\"{feature}_SHAP_train\"] = shap_array[:, i]\n",
        "\n",
        "# --- Step 5: Visualize SHAP for first example ---\n",
        "print(\"SHAP Waterfall Plot for Test Example:\")\n",
        "shap.plots.waterfall(shap_values[0])\n",
        "plt.show()\n",
        "\n",
        "#print(\"SHAP Waterfall Plot for Train Example:\")\n",
        "#shap.plots.waterfall(shap_values_train[0])\n",
        "#plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Eu4oIUIVVg8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df.columns"
      ],
      "metadata": {
        "id": "zXfNFXkocj3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(shap_values, X_test_cls)\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z72kE89iVkzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shap.summary_plot(shap_values_train, X_train_cls)\n",
        "#plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ryGQelvbVrsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 8: SHAP Waterfall Plot for Specific Animal ---\n",
        "animal_id = \"A1354267\"  # Replace with your desired animal ID\n",
        "\n",
        "# Use the final decoded dataframe\n",
        "decoded_df = combined_df.copy()\n",
        "\n",
        "# Get all matching rows for the specified animal ID\n",
        "matching_rows = decoded_df[decoded_df['animal_id'].astype(str) == animal_id]\n",
        "\n",
        "if matching_rows.empty:\n",
        "    print(f\"No records found for Animal ID: {animal_id}\")\n",
        "else:\n",
        "    for idx, row in matching_rows.iterrows():\n",
        "        # Extract SHAP columns and values\n",
        "        shap_cols = [col for col in decoded_df.columns if col.endswith('_SHAP')]\n",
        "        shap_values = row[shap_cols].values\n",
        "\n",
        "        # Base value (mean model prediction)\n",
        "        base_value = decoded_df['xgb_predicted_adopt_score'].mean()\n",
        "\n",
        "        # Get original feature names and values\n",
        "        feature_names = [col.replace('_SHAP', '') for col in shap_cols]\n",
        "        feature_values = row[feature_names].values\n",
        "\n",
        "        # Create SHAP explanation object\n",
        "        shap_explanation = shap.Explanation(\n",
        "            values=shap_values,\n",
        "            base_values=base_value,\n",
        "            data=feature_values,\n",
        "            feature_names=feature_names\n",
        "        )\n",
        "\n",
        "        # Print info and plot\n",
        "        print(f\"\\nðŸ¾ Waterfall Plot for Animal ID: {animal_id} (Index: {idx})\")\n",
        "        shap.plots.waterfall(shap_explanation)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "oSo5uNCLVuWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "animal_id = \"A1354267\"  # Replace with any valid animal_id\n",
        "\n",
        "# Combine feature list\n",
        "numerical_features_clf = [\n",
        "    'Num_returned', 'age_months', 'stay_length_days', 'min_height', 'max_height',\n",
        "    'min_weight', 'max_weight', 'min_expectancy', 'max_expectancy',\n",
        "    'grooming_frequency_value', 'shedding_value', 'energy_level_value',\n",
        "    'trainability_value', 'demeanor_value'\n",
        "]\n",
        "categorical_columns = [\n",
        "    'animal_type', 'primary_breed_harmonized', 'primary_color_harmonized',\n",
        "    'sex', 'intake_type_harmonized', 'Is_returned', 'has_name', 'is_mix', 'shelter'\n",
        "]\n",
        "all_feature_names = numerical_features_clf + categorical_columns\n",
        "\n",
        "# Filter rows for the animal_id\n",
        "animal_rows = decoded_df[decoded_df[\"animal_id\"].astype(str) == animal_id]\n",
        "\n",
        "if animal_rows.empty:\n",
        "    print(f\"No records found for Animal ID: {animal_id}\")\n",
        "else:\n",
        "    for idx, row in animal_rows.iterrows():\n",
        "        adoption_score = row['xgb_predicted_adopt_score']\n",
        "        predicted_stay = row.get('predicted_stay_length_days_rf', np.nan)\n",
        "\n",
        "        shap_cols = [col for col in decoded_df.columns if col.endswith('_SHAP')]\n",
        "        shap_values = row[shap_cols].values\n",
        "        features = [col.replace('_SHAP', '') for col in shap_cols]\n",
        "        feature_values = row[features].values\n",
        "\n",
        "        shap_df = pd.DataFrame({\n",
        "            \"feature\": features,\n",
        "            \"value\": feature_values,\n",
        "            \"shap\": shap_values\n",
        "        })\n",
        "\n",
        "        # Select top 3 contributors based on predicted_stay_length_days logic\n",
        "        if adoption_score >= 50:\n",
        "            top_contributors = shap_df[shap_df[\"shap\"] > 0].sort_values(by=\"shap\", ascending=False).head(3)\n",
        "        else:\n",
        "            top_contributors = shap_df[shap_df[\"shap\"] < 0].reindex(\n",
        "                shap_df[shap_df[\"shap\"] < 0][\"shap\"].abs().sort_values(ascending=False).index\n",
        "            ).head(3)\n",
        "\n",
        "        print(f\"\\n===============================\")\n",
        "        print(f\"Animal ID: {animal_id} | Record Index: {idx}\")\n",
        "        print(f\"Adoption Score: {adoption_score:.2f}\")\n",
        "        print(f\"Predicted Stay Length: {predicted_stay:.2f}\")\n",
        "\n",
        "        print(\"\\nðŸ” Top 3 SHAP Contributors:\")\n",
        "        for _, row_contrib in top_contributors.iterrows():\n",
        "            impact = \"â†‘\" if row_contrib[\"shap\"] > 0 else \"â†“\"\n",
        "            print(f\"  â€¢ {row_contrib['feature']} = {row_contrib['value']} ({impact} impact of {row_contrib['shap']:.2f})\")\n"
      ],
      "metadata": {
        "id": "Y_kjfYcdVxEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Multiclass AUC Curve ---\n",
        "# Encode labels and binarize for ROC\n",
        "\n",
        "y_test_binarized = label_binarize(y_test_encoded, classes=range(len(label_encoder.classes_)))\n",
        "y_score_rf = rf_na_model.predict_proba(X_test_na)\n",
        "y_pred_rf_encoded = label_encoder.transform(y_pred_test_na)\n",
        "y_pred_xgb_encoded = y_pred_test_xgb_na  # Already encoded since XGB was trained on y_train_encoded\n",
        "y_score_xgb = y_proba_test_xgb_na\n",
        "\n",
        "\n",
        "# ROC Curve for each class\n",
        "fpr, tpr, roc_auc = {}, {}, {}\n",
        "for i in range(len(label_encoder.classes_)):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_score_rf[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot ROC Curves\n",
        "plt.figure(figsize=(10, 7))\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple'])\n",
        "for i, color in zip(range(len(label_encoder.classes_)), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "             label=f\"{label_encoder.classes_[i]} (AUC = {roc_auc[i]:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Non-Adopted Outcome Classifier (RF)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# --- Compare with Random Forest ---\n",
        "auc_rf = roc_auc_score(y_test_binarized, y_score_rf, average='macro', multi_class='ovr')\n",
        "print(\"\\n Random Forest Classification Report:\")\n",
        "print(classification_report(y_test_encoded, y_pred_rf_encoded, target_names=label_encoder.classes_))\n",
        "print(f\"Random Forest Macro AUC: {auc_rf:.3f}\")\n",
        "\n",
        "auc_xgb = roc_auc_score(y_test_binarized, y_score_xgb, average='macro', multi_class='ovr')\n",
        "print(\"\\n XGBoost Classification Report:\")\n",
        "print(classification_report(y_test_encoded, y_pred_xgb_encoded, target_names=label_encoder.classes_))\n",
        "print(f\"XGBoost Macro AUC: {auc_xgb:.3f}\")"
      ],
      "metadata": {
        "id": "xhStCJ0RJ-Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df.columns"
      ],
      "metadata": {
        "id": "EW1vLilIpKrm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}