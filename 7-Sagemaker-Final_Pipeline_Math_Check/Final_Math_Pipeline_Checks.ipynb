{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184655dd-7c4d-453e-b0ca-188ea4159a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is final check file to make sure math is working correctly and is similar to originally tested models from team in python\n",
    "#from ShelterData_ML_Final.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e11d6-9607-4991-b738-ff257829f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Update  code here to test different csvs, we're using our test data set\n",
    "import shutil; shutil.copyfile('final_pipeline_prediction_sample_Test.csv', 'final_pipeline_prediction.csv')\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score,\n",
    "    recall_score, f1_score, roc_auc_score, roc_curve, classification_report)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee293741-d354-4b73-868b-c795765819ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Adoption Prediciton first\n",
    "\n",
    "\n",
    "#load\n",
    "df_truth = pd.read_csv('df_cat_dog_harmonized.csv') # This is ground truth file\n",
    "df_pred = pd.read_csv('final_pipeline_prediction.csv')\n",
    "\n",
    "#Merge\n",
    "df = pd.merge(df_truth[['primary_key','outcome_type_harmonized', 'outcome_type_harmonized_grouped', 'stay_length_days']], \n",
    "              df_pred[['primary_key','predicted_label', 'predicted_proba','stay_length_predicted', 'non_adopted_label']], \n",
    "              on='primary_key')\n",
    "\n",
    "#Binarize ground truth: adopted = 1, else 0\n",
    "\n",
    "df['actual_adopt'] = df['outcome_type_harmonized_grouped'].apply(lambda x: 1 if str(x).strip().lower() == 'adopted' else 0)\n",
    "\n",
    "# Metrics\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(df['actual_adopt'], df['predicted_label']))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(df['actual_adopt'], df['predicted_label'], labels=[0, 1], target_names=[\"Not Adopted\", \"Adopted\"]))\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(df['actual_adopt'], df['predicted_label']):.4f}\")\n",
    "print(f\"Precision: {precision_score(df['actual_adopt'], df['predicted_label']):.4f}\")\n",
    "print(f\"Recall: {recall_score(df['actual_adopt'], df['predicted_label']):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(df['actual_adopt'], df['predicted_label']):.4f}\")\n",
    "\n",
    "\n",
    "# AUC\n",
    "\n",
    "if 'predicted_proba' in df.columns:\n",
    "    auc = roc_auc_score(df['actual_adopt'], df['predicted_proba'])\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "#Plot ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(df['actual_adopt'], df['predicted_proba'])\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel(\"False Positive Rate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183f6f1-2d38-4324-85e0-31cbcfaa3848",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf904751-8fd9-427c-80ef-3fda70915dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighted Accuracy is good and AUC is good, so we keep model as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f7289-6887-4e49-9eb2-2189182fa64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking predicted stay lenght but only for adopted \n",
    "\n",
    "#  only include rows where predicted_proba >= 0.5\n",
    "df_filtered = df[df['predicted_proba'] >= 0.5].copy()\n",
    "\n",
    "# Drop rows with missing values in either column\n",
    "df_filtered = df_filtered.dropna(subset=['stay_length_days', 'stay_length_predicted'])\n",
    "\n",
    "# Evaluate metrics\n",
    "r2 = r2_score(df_filtered['stay_length_days'], df_filtered['stay_length_predicted'])\n",
    "mse = mean_squared_error(df_filtered['stay_length_days'], df_filtered['stay_length_predicted'])\n",
    "mae = mean_absolute_error(df_filtered['stay_length_days'], df_filtered['stay_length_predicted'])\n",
    "\n",
    "print(f\"Filtered Rows: {len(df_filtered)}\")\n",
    "print(f\"RÂ² Score: {r2:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d029be-5c66-41e2-8671-4253cac3cc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#R2 is pretty similar to what we expected, we ended up dropping this.\n",
    "#Now we evlauate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6af7dc-b6a4-4b39-a64b-328f10fceaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and map ground truth to integer labels\n",
    "label_map = {\n",
    "    \"foster\": 0,\n",
    "    \"foster to adopt\": 0,\n",
    "    \"rtf\": 0,\n",
    "    \"rescue\": 1,\n",
    "    \"return to rescue\": 1,\n",
    "    \"return to owner\": 2\n",
    "}\n",
    "df['outcome_type_harmonized'] = df['outcome_type_harmonized'].str.lower().str.strip()\n",
    "df['actual_label'] = df['outcome_type_harmonized'].map(label_map)\n",
    "df['predicted_label'] = df['non_adopted_label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26609432-fddd-4f9a-9104-b150aa4d816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3754c-e26f-40e3-8098-59696df7f232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lastly, we'll predict multiclass outcome\n",
    "\n",
    "\n",
    "# Drop rows with unknown classes \n",
    "df = df.dropna(subset=['actual_label'])\n",
    "\n",
    "# Keep only rows where predicted adoption confidence is low\n",
    "df = df[df['predicted_proba'] < 0.5]\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "print(\"Unique ground truth labels:\")\n",
    "print(df['outcome_type_harmonized'].unique())\n",
    "\n",
    "print(\"\\nValue counts after mapping:\")\n",
    "print(df['actual_label'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(df['actual_label'], df['predicted_label']))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(df['actual_label'], df['predicted_label'],\n",
    "                            target_names=['foster', 'rescue', 'return_to_owner']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31afb84f-07eb-45bf-95aa-b473f51b9d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f36eb-7622-4c1a-bba3-be1359300994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
